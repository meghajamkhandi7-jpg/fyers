#!/usr/bin/env python3
"""
Backfill task_id and task_completion_time_seconds into existing balance.jsonl files.

Key insight: an agent may have been run multiple times (restarted), causing the same
simulation date to appear more than once in tasks.jsonl and balance.jsonl.  Each run
still assigns exactly ONE task per day.  We match entries positionally: the Nth
balance record for a given date corresponds to the Nth task record for that date.

  task_id
    The UUID of the task assigned on that day (string).

  task_completion_time_seconds
    Wall-clock seconds from "Task state set successfully" to
    "Submitting work for evaluation" for that task_id, derived from
    logs/info.jsonl.  Null when the agent chose to learn (no submission).

Entries with date "initialization" or already having task_id set are skipped.
Run from repo root:
    python scripts/backfill_balance_task_info.py
"""
import json
from collections import defaultdict
from datetime import datetime
from pathlib import Path

REPO_ROOT = Path(__file__).parent.parent
DATA_PATH = REPO_ROOT / "livebench" / "data" / "agent_data"


def read_jsonl(path: Path) -> list:
    if not path.exists():
        return []
    entries = []
    with open(path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    entries.append(json.loads(line))
                except json.JSONDecodeError:
                    pass
    return entries


def write_jsonl(path: Path, records: list) -> None:
    with open(path, "w", encoding="utf-8") as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")


def build_date_to_task_ids_ordered(agent_dir: Path) -> dict:
    """
    Return {date: [task_id, task_id, ...]} preserving file order.
    Multiple entries for the same date come from repeated simulation runs.
    """
    mapping: dict[str, list] = defaultdict(list)
    for entry in read_jsonl(agent_dir / "work" / "tasks.jsonl"):
        date = entry.get("date")
        tid  = entry.get("task_id")
        if date and tid:
            mapping[date].append(tid)
    return dict(mapping)


def build_task_durations(agent_dir: Path) -> dict:
    """
    Return {task_id: seconds} using logs/info.jsonl.

    A task may be started more than once if the agent was restarted mid-run.
    We collect ALL start timestamps per task_id, then for each submission
    ("Submitting work for evaluation") we find the LATEST start that occurred
    BEFORE that submission — giving the actual wall-clock of the final attempt.
    """
    # {task_id: [dt, dt, ...]} — all start events in file order
    all_starts: dict[str, list] = {}
    # {task_id: dt} — first submission event
    ends: dict[str, datetime] = {}

    for entry in read_jsonl(agent_dir / "logs" / "info.jsonl"):
        msg = entry.get("message", "")
        ctx = entry.get("context", {}) or {}
        tid = ctx.get("task_id")
        ts  = entry.get("timestamp")
        if not (tid and ts):
            continue
        try:
            dt = datetime.fromisoformat(ts)
        except ValueError:
            continue

        if msg == "Task state set successfully":
            all_starts.setdefault(tid, []).append(dt)
        elif msg == "Submitting work for evaluation" and tid not in ends:
            ends[tid] = dt

    durations = {}
    for tid, end_dt in ends.items():
        starts_before = [s for s in all_starts.get(tid, []) if s <= end_dt]
        if starts_before:
            durations[tid] = (end_dt - max(starts_before)).total_seconds()
    return durations


def backfill_agent(agent_dir: Path) -> int:
    balance_file = agent_dir / "economic" / "balance.jsonl"
    if not balance_file.exists():
        return 0

    records     = read_jsonl(balance_file)
    date_to_ids = build_date_to_task_ids_ordered(agent_dir)
    durations   = build_task_durations(agent_dir)

    # Track how many times we've seen each date so far (positional matching)
    date_seen: dict[str, int] = defaultdict(int)

    updated = 0
    for rec in records:
        date = rec.get("date")
        if not date or date == "initialization":
            continue
        if rec.get("task_id") is not None:
            # Already populated — still advance the counter
            date_seen[date] += 1
            continue

        idx  = date_seen[date]
        tids = date_to_ids.get(date, [])

        if idx < len(tids):
            tid = tids[idx]
            rec["task_id"] = tid
            rec["task_completion_time_seconds"] = durations.get(tid)  # None if learn day
            updated += 1

        date_seen[date] += 1

    if updated:
        write_jsonl(balance_file, records)
    return updated


def main():
    if not DATA_PATH.exists():
        print(f"Data path not found: {DATA_PATH}")
        return

    agent_dirs = [d for d in sorted(DATA_PATH.iterdir()) if d.is_dir()]
    total_updated = 0
    for agent_dir in agent_dirs:
        n = backfill_agent(agent_dir)
        if n:
            print(f"  {agent_dir.name}: updated {n} record(s)")
        else:
            print(f"  {agent_dir.name}: nothing to update")
        total_updated += n

    print(f"\nDone — {total_updated} balance record(s) updated across {len(agent_dirs)} agent(s).")


if __name__ == "__main__":
    main()
