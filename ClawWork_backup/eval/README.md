# Meta-Prompt Generation for LLM-Based Evaluation

Generate evaluation prompts for 44 task categories in the gdpval dataset. Each meta-prompt guides LLMs to evaluate task outputs on a **0-10 scale**, with heavy penalties for missing or incomplete artifacts.

## Quick Start

```bash
# 1. Setup
export OPENAI_API_KEY='your-key'
pip install pandas pyarrow openai

# 2. Test single category (30 seconds, ~$0.05)
python test_single_category.py

# 3. Generate all 44 categories (90-120 min, ~$1-2)
python generate_meta_prompts.py

# Note: Script automatically skips already-generated categories
# Safe to re-run if interrupted - it will resume where it left off
```

## Output

- `meta_prompts/{category}.json` - 44 evaluation prompt files
- `meta_prompts/generation_summary.json` - Summary report
- `meta_prompt_generation.log` - Detailed log

## Scoring Scale (0-10)

- **9-10**: Excellent, complete, professional quality
- **7-8**: Good, minor issues
- **5-6**: Acceptable, notable gaps
- **3-4**: Poor, major issues
- **0-2**: Unacceptable, missing artifacts or severely incomplete

**Key**: Missing required output files = automatic score â‰¤2

