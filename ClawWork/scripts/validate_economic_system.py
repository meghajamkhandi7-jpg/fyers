"""
Comprehensive validation script for the improved Economic Tracker system

This script:
1. Validates the new record format in both files
2. Checks all integration points
3. Creates example data showing the improvements
4. Validates evaluation threshold logic
"""

import os
import sys
import json
import tempfile
import shutil
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from livebench.agent.economic_tracker import EconomicTracker


def demo_new_format():
    """Demonstrate the new format with actual examples"""
    print("\n" + "="*70)
    print("DEMONSTRATION: New Economic Tracker Format")
    print("="*70)
    
    temp_dir = tempfile.mkdtemp()
    
    try:
        tracker = EconomicTracker(
            signature="demo-agent",
            initial_balance=1000.0,
            data_path=temp_dir,
            min_evaluation_threshold=0.6
        )
        tracker.initialize()
        
        print("\nüìã Scenario: Agent works on 3 tasks in one day")
        print("   - Task 1: High quality (score 0.85) - Gets paid")
        print("   - Task 2: Low quality (score 0.45) - No payment")
        print("   - Task 3: Acceptable quality (score 0.65) - Gets paid")
        
        date = "2026-01-21"
        completed_tasks = []
        total_income = 0.0
        
        # Task 1: High quality
        print("\n" + "-"*70)
        print("TASK 1: Writing technical documentation (High Quality)")
        print("-"*70)
        tracker.start_task("task-2026-01-21-doc", date=date)
        
        # Simulate work
        tracker.track_tokens(2500, 1200)  # Main work
        tracker.track_api_call(150, 0.05, "JINA_Search")  # Research
        tracker.track_tokens(500, 300)  # Refinement
        
        # Submit and evaluate
        payment1 = tracker.add_work_income(
            amount=45.0,
            task_id="task-2026-01-21-doc",
            evaluation_score=0.85,
            description="Technical documentation completed"
        )
        total_income += payment1
        completed_tasks.append("task-2026-01-21-doc")
        tracker.end_task()
        
        # Task 2: Low quality
        print("\n" + "-"*70)
        print("TASK 2: Data analysis report (Low Quality)")
        print("-"*70)
        tracker.start_task("task-2026-01-21-analysis", date=date)
        
        # Simulate work
        tracker.track_tokens(1500, 800)  # Incomplete work
        tracker.track_api_call(500, 0.0417, "OCR_Input")  # Process document
        
        # Submit and evaluate
        payment2 = tracker.add_work_income(
            amount=40.0,
            task_id="task-2026-01-21-analysis",
            evaluation_score=0.45,  # Below threshold!
            description="Data analysis submitted but incomplete"
        )
        total_income += payment2
        completed_tasks.append("task-2026-01-21-analysis")
        tracker.end_task()
        
        # Task 3: Acceptable quality
        print("\n" + "-"*70)
        print("TASK 3: Code review (Acceptable Quality)")
        print("-"*70)
        tracker.start_task("task-2026-01-21-review", date=date)
        
        # Simulate work
        tracker.track_tokens(1800, 900)
        tracker.track_api_call(80, 0.05, "JINA_Search")
        tracker.track_tokens(300, 150)  # Follow-up
        
        # Submit and evaluate
        payment3 = tracker.add_work_income(
            amount=35.0,
            task_id="task-2026-01-21-review",
            evaluation_score=0.65,
            description="Code review completed"
        )
        total_income += payment3
        completed_tasks.append("task-2026-01-21-review")
        tracker.end_task()
        
        # Save daily state
        tracker.save_daily_state(
            date=date,
            work_income=total_income,
            trading_profit=0.0,
            completed_tasks=completed_tasks
        )
        
        # Display results
        print("\n" + "="*70)
        print("DAILY SUMMARY")
        print("="*70)
        
        summary = tracker.get_daily_summary(date)
        print(f"\nDate: {summary['date']}")
        print(f"Tasks worked on: {len(summary['tasks'])}")
        print(f"Tasks completed: {summary['tasks_completed']}")
        print(f"Tasks paid: {summary['tasks_paid']}")
        print(f"Tasks rejected (low quality): {summary['tasks_completed'] - summary['tasks_paid']}")
        
        print(f"\nCosts by channel:")
        print(f"  LLM tokens:  ${summary['costs']['llm_tokens']:.4f}")
        print(f"  Search API:  ${summary['costs']['search_api']:.4f}")
        print(f"  OCR API:     ${summary['costs']['ocr_api']:.4f}")
        print(f"  Other API:   ${summary['costs']['other_api']:.4f}")
        print(f"  TOTAL COST:  ${summary['costs']['total']:.4f}")
        
        print(f"\nFinancial:")
        print(f"  Work income: ${summary['work_income']:.2f}")
        print(f"  Net profit:  ${summary['work_income'] - summary['costs']['total']:.2f}")
        print(f"  Final balance: ${tracker.get_balance():.2f}")
        
        # Show individual task costs
        print(f"\n" + "="*70)
        print("PER-TASK COST BREAKDOWN")
        print("="*70)
        
        for task_id in summary['tasks']:
            costs = tracker.get_task_costs(task_id)
            print(f"\n{task_id}:")
            print(f"  LLM: ${costs['llm_tokens']:.4f}, "
                  f"Search: ${costs['search_api']:.4f}, "
                  f"OCR: ${costs['ocr_api']:.4f}, "
                  f"Total: ${costs['total']:.4f}")
        
        # Show sample records
        print(f"\n" + "="*70)
        print("SAMPLE RECORDS FROM token_costs.jsonl")
        print("="*70)
        
        token_costs_file = os.path.join(temp_dir, "token_costs.jsonl")
        with open(token_costs_file, "r") as f:
            lines = f.readlines()
        
        # Show different record types
        print("\n1Ô∏è‚É£  LLM Token Record:")
        for line in lines:
            rec = json.loads(line)
            if rec["type"] == "llm_tokens":
                print(json.dumps(rec, indent=2))
                break
        
        print("\n2Ô∏è‚É£  API Call Record (Search):")
        for line in lines:
            rec = json.loads(line)
            if rec["type"] == "api_call" and rec.get("channel") == "search_api":
                print(json.dumps(rec, indent=2))
                break
        
        print("\n3Ô∏è‚É£  API Call Record (OCR):")
        for line in lines:
            rec = json.loads(line)
            if rec["type"] == "api_call" and rec.get("channel") == "ocr_api":
                print(json.dumps(rec, indent=2))
                break
        
        print("\n4Ô∏è‚É£  Work Income Record (Payment Awarded):")
        for line in lines:
            rec = json.loads(line)
            if rec["type"] == "work_income" and rec.get("payment_awarded"):
                print(json.dumps(rec, indent=2))
                break
        
        print("\n5Ô∏è‚É£  Work Income Record (Payment Rejected - Low Quality):")
        for line in lines:
            rec = json.loads(line)
            if rec["type"] == "work_income" and not rec.get("payment_awarded"):
                print(json.dumps(rec, indent=2))
                break
        
        print("\n6Ô∏è‚É£  Task Summary Record:")
        for line in lines:
            rec = json.loads(line)
            if rec["type"] == "task_summary":
                print(json.dumps(rec, indent=2))
                break
        
        print("\n7Ô∏è‚É£  Daily Balance Record:")
        balance_file = os.path.join(temp_dir, "balance.jsonl")
        with open(balance_file, "r") as f:
            lines = f.readlines()
        last_record = json.loads(lines[-1])
        print(json.dumps(last_record, indent=2))
        
        print("\n" + "="*70)
        print("‚úÖ DEMONSTRATION COMPLETE")
        print("="*70)
        
    finally:
        shutil.rmtree(temp_dir)


def validate_integration_points():
    """Validate all code integration points"""
    print("\n" + "="*70)
    print("VALIDATION: Code Integration Points")
    print("="*70)
    
    checks = []
    
    # Check 1: EconomicTracker has new methods
    from livebench.agent.economic_tracker import EconomicTracker
    
    required_methods = [
        'start_task',
        'end_task',
        'add_work_income',
        'get_task_costs',
        'get_daily_summary',
        'get_cost_analytics'
    ]
    
    for method in required_methods:
        has_method = hasattr(EconomicTracker, method)
        checks.append(("EconomicTracker." + method, has_method))
        print(f"{'‚úì' if has_method else '‚úó'} EconomicTracker.{method}()")
    
    # Check 2: add_work_income signature
    import inspect
    sig = inspect.signature(EconomicTracker.add_work_income)
    params = list(sig.parameters.keys())
    has_eval_score = 'evaluation_score' in params
    checks.append(("add_work_income has evaluation_score param", has_eval_score))
    print(f"{'‚úì' if has_eval_score else '‚úó'} add_work_income() requires evaluation_score parameter")
    
    # Check 3: evaluate_artifact returns 4 values
    from livebench.work.evaluator import WorkEvaluator
    eval_sig = inspect.signature(WorkEvaluator.evaluate_artifact)
    return_annotation = eval_sig.return_annotation
    returns_4_values = "float, float" in str(return_annotation) or "Tuple[bool, float, str, float]" in str(return_annotation)
    checks.append(("evaluate_artifact returns 4 values", returns_4_values))
    print(f"{'‚úì' if returns_4_values else '‚úó'} evaluate_artifact() returns (accepted, payment, feedback, score)")
    
    # Check 4: Check live_agent.py integration
    live_agent_file = Path(__file__).parent.parent / "livebench" / "agent" / "live_agent.py"
    with open(live_agent_file, "r") as f:
        live_agent_code = f.read()
    
    has_start_task = "start_task" in live_agent_code
    has_end_task = "end_task" in live_agent_code
    has_actual_payment = "actual_payment" in live_agent_code
    
    checks.append(("live_agent.py calls start_task()", has_start_task))
    checks.append(("live_agent.py calls end_task()", has_end_task))
    checks.append(("live_agent.py uses actual_payment", has_actual_payment))
    
    print(f"{'‚úì' if has_start_task else '‚úó'} live_agent.py calls start_task()")
    print(f"{'‚úì' if has_end_task else '‚úó'} live_agent.py calls end_task()")
    print(f"{'‚úì' if has_actual_payment else '‚úó'} live_agent.py tracks actual_payment")
    
    # Check 5: direct_tools.py integration
    direct_tools_file = Path(__file__).parent.parent / "livebench" / "tools" / "direct_tools.py"
    with open(direct_tools_file, "r") as f:
        direct_tools_code = f.read()
    
    has_eval_score_param = "evaluation_score" in direct_tools_code
    checks.append(("direct_tools.py passes evaluation_score", has_eval_score_param))
    print(f"{'‚úì' if has_eval_score_param else '‚úó'} direct_tools.py passes evaluation_score to add_work_income()")
    
    # Summary
    all_passed = all(check[1] for check in checks)
    
    print("\n" + "="*70)
    if all_passed:
        print("‚úÖ ALL INTEGRATION CHECKS PASSED")
    else:
        print("‚ùå SOME INTEGRATION CHECKS FAILED:")
        for name, passed in checks:
            if not passed:
                print(f"   ‚úó {name}")
    print("="*70)
    
    return all_passed


def validate_threshold_logic():
    """Validate the 0.6 threshold logic comprehensively"""
    print("\n" + "="*70)
    print("VALIDATION: Evaluation Threshold Logic")
    print("="*70)
    
    temp_dir = tempfile.mkdtemp()
    
    try:
        tracker = EconomicTracker(
            signature="threshold-test",
            initial_balance=1000.0,
            data_path=temp_dir,
            min_evaluation_threshold=0.6
        )
        tracker.initialize()
        
        test_scenarios = [
            # (score, base_payment, expected_payment, description)
            (0.0, 50.0, 0.0, "Zero quality - complete failure"),
            (0.3, 50.0, 0.0, "Poor quality - major issues"),
            (0.59, 50.0, 0.0, "Below threshold by 0.01"),
            (0.5999, 50.0, 0.0, "Below threshold by 0.0001"),
            (0.6, 50.0, 50.0, "Exactly at threshold - minimum acceptable"),
            (0.6001, 50.0, 50.0, "Just above threshold"),
            (0.7, 50.0, 50.0, "Good quality"),
            (0.85, 50.0, 50.0, "Very good quality"),
            (1.0, 50.0, 50.0, "Perfect score"),
        ]
        
        results = []
        for i, (score, base, expected, desc) in enumerate(test_scenarios):
            tracker.start_task(f"task-{i}", date="2026-01-21")
            tracker.track_tokens(1000, 500)  # Some work
            
            actual = tracker.add_work_income(
                amount=base,
                task_id=f"task-{i}",
                evaluation_score=score,
                description=desc
            )
            tracker.end_task()
            
            passed = actual == expected
            results.append((score, base, expected, actual, passed, desc))
            
            status = "‚úì" if passed else "‚úó"
            print(f"{status} Score {score:.4f}: ${actual:.2f} (expected ${expected:.2f}) - {desc}")
        
        # Validate payment logic in records
        token_costs_file = os.path.join(temp_dir, "token_costs.jsonl")
        work_income_records = []
        
        with open(token_costs_file, "r") as f:
            for line in f:
                rec = json.loads(line)
                if rec["type"] == "work_income":
                    work_income_records.append(rec)
        
        print(f"\n‚úì Found {len(work_income_records)} work income records")
        
        # Validate each record
        for rec in work_income_records:
            score = rec["evaluation_score"]
            threshold = rec["threshold"]
            payment_awarded = rec["payment_awarded"]
            actual_payment = rec["actual_payment"]
            
            # Logic check
            should_be_paid = score >= threshold
            logic_correct = payment_awarded == should_be_paid
            amount_correct = (actual_payment > 0) == payment_awarded
            
            if not logic_correct or not amount_correct:
                print(f"‚úó Logic error in record: score={score}, threshold={threshold}, "
                      f"awarded={payment_awarded}, payment={actual_payment}")
                return False
        
        print(f"‚úì All {len(work_income_records)} records have correct threshold logic")
        
        all_passed = all(r[4] for r in results)
        
        print("\n" + "="*70)
        if all_passed:
            print("‚úÖ THRESHOLD LOGIC VALIDATION PASSED")
        else:
            print("‚ùå THRESHOLD LOGIC VALIDATION FAILED")
        print("="*70)
        
        return all_passed
        
    finally:
        shutil.rmtree(temp_dir)


def validate_cost_channel_separation():
    """Validate that costs are properly separated by channel"""
    print("\n" + "="*70)
    print("VALIDATION: Cost Channel Separation")
    print("="*70)
    
    temp_dir = tempfile.mkdtemp()
    
    try:
        tracker = EconomicTracker(
            signature="channel-test",
            initial_balance=1000.0,
            data_path=temp_dir
        )
        tracker.initialize()
        
        # Simulate mixed API usage
        tracker.start_task("task-mixed-apis", date="2026-01-21")
        
        # Different types of calls
        llm_cost = tracker.track_tokens(3000, 1500)
        search_cost = tracker.track_api_call(200, 0.05, "JINA_Search")
        ocr_cost1 = tracker.track_api_call(1000, 0.0417, "OCR_Input")
        ocr_cost2 = tracker.track_api_call(500, 0.0694, "OCR_Output")
        other_cost = tracker.track_api_call(100, 0.1, "Custom_API")
        
        tracker.end_task()
        
        # Verify task costs
        task_costs = tracker.get_task_costs("task-mixed-apis")
        
        print(f"\n‚úì Cost breakdown for task-mixed-apis:")
        print(f"  LLM tokens:  ${task_costs['llm_tokens']:.6f} (expected ~${llm_cost:.6f})")
        print(f"  Search API:  ${task_costs['search_api']:.6f} (expected ~${search_cost:.6f})")
        print(f"  OCR API:     ${task_costs['ocr_api']:.6f} (expected ~${ocr_cost1 + ocr_cost2:.6f})")
        print(f"  Other API:   ${task_costs['other_api']:.6f} (expected ~${other_cost:.6f})")
        print(f"  Total:       ${task_costs['total']:.6f}")
        
        # Validate
        tolerance = 0.000001
        checks = [
            (abs(task_costs['llm_tokens'] - llm_cost) < tolerance, "LLM cost"),
            (abs(task_costs['search_api'] - search_cost) < tolerance, "Search API cost"),
            (abs(task_costs['ocr_api'] - (ocr_cost1 + ocr_cost2)) < tolerance, "OCR API cost"),
            (abs(task_costs['other_api'] - other_cost) < tolerance, "Other API cost"),
        ]
        
        all_passed = all(check[0] for check in checks)
        
        for passed, name in checks:
            print(f"{'‚úì' if passed else '‚úó'} {name} correctly tracked")
        
        print("\n" + "="*70)
        if all_passed:
            print("‚úÖ COST CHANNEL SEPARATION VALIDATED")
        else:
            print("‚ùå COST CHANNEL SEPARATION FAILED")
        print("="*70)
        
        return all_passed
        
    finally:
        shutil.rmtree(temp_dir)


def validate_query_capabilities():
    """Validate querying capabilities"""
    print("\n" + "="*70)
    print("VALIDATION: Query Capabilities")
    print("="*70)
    
    temp_dir = tempfile.mkdtemp()
    
    try:
        tracker = EconomicTracker(
            signature="query-test",
            initial_balance=1000.0,
            data_path=temp_dir
        )
        tracker.initialize()
        
        # Create data across multiple dates and tasks
        dates = ["2026-01-20", "2026-01-21", "2026-01-22"]
        all_tasks = []
        
        for date in dates:
            for i in range(2):
                task_id = f"task-{date}-{i}"
                all_tasks.append(task_id)
                
                tracker.start_task(task_id, date=date)
                tracker.track_tokens(1000 * (i+1), 500 * (i+1))
                tracker.track_api_call(50 * (i+1), 0.05, "JINA_Search")
                
                # Alternate between paid and unpaid
                score = 0.7 if i == 0 else 0.5
                tracker.add_work_income(30.0, task_id, score)
                tracker.end_task()
        
        # Test 1: Query by specific task
        print("\n1Ô∏è‚É£  Query by Task ID:")
        task_costs = tracker.get_task_costs("task-2026-01-21-0")
        print(f"   Task task-2026-01-21-0: Total cost ${task_costs['total']:.6f}")
        assert task_costs['total'] > 0, "Task should have costs"
        print("   ‚úì Task-specific query works")
        
        # Test 2: Query by date
        print("\n2Ô∏è‚É£  Query by Date:")
        for date in dates:
            daily = tracker.get_daily_summary(date)
            print(f"   {date}: {len(daily['tasks'])} tasks, "
                  f"${daily['costs']['total']:.6f} cost, "
                  f"${daily['work_income']:.2f} income, "
                  f"{daily['tasks_paid']}/{daily['tasks_completed']} paid")
            
            # Each date should have 2 tasks, 1 paid
            assert len(daily['tasks']) == 2, f"Should have 2 tasks on {date}"
            assert daily['tasks_completed'] == 2, "Should have 2 completed tasks"
            assert daily['tasks_paid'] == 1, "Should have 1 paid task"
        
        print("   ‚úì Date-based queries work")
        
        # Test 3: Overall analytics
        print("\n3Ô∏è‚É£  Overall Analytics:")
        analytics = tracker.get_cost_analytics()
        print(f"   Total tasks: {analytics['total_tasks']}")
        print(f"   Tasks paid: {analytics['tasks_paid']}")
        print(f"   Tasks rejected: {analytics['tasks_rejected']}")
        print(f"   Total costs: ${analytics['total_costs']['total']:.6f}")
        print(f"   Total income: ${analytics['total_income']:.2f}")
        
        assert analytics['total_tasks'] == 6, "Should have 6 total tasks"
        assert analytics['tasks_paid'] == 3, "Should have 3 paid tasks"
        assert analytics['tasks_rejected'] == 3, "Should have 3 rejected tasks"
        print("   ‚úì Analytics aggregation works")
        
        # Test 4: Verify data by different dimensions
        print("\n4Ô∏è‚É£  Multi-dimensional Analysis:")
        by_date_count = len(analytics['by_date'])
        by_task_count = len(analytics['by_task'])
        
        print(f"   Unique dates: {by_date_count}")
        print(f"   Unique tasks: {by_task_count}")
        
        assert by_date_count == 3, "Should have 3 dates"
        assert by_task_count == 6, "Should have 6 tasks"
        print("   ‚úì Can analyze from multiple dimensions")
        
        print("\n" + "="*70)
        print("‚úÖ QUERY CAPABILITIES VALIDATED")
        print("="*70)
        
        return True
        
    finally:
        shutil.rmtree(temp_dir)


def check_backward_compatibility_notes():
    """Note backward compatibility considerations"""
    print("\n" + "="*70)
    print("BACKWARD COMPATIBILITY NOTES")
    print("="*70)
    
    print("\n‚ö†Ô∏è  Breaking Changes:")
    print("   1. add_work_income() now REQUIRES evaluation_score parameter")
    print("   2. evaluate_artifact() now RETURNS 4 values (added evaluation_score)")
    print("   3. start_task() should be called when task begins")
    print("   4. end_task() should be called when task completes/fails")
    
    print("\nüìù Migration Required For:")
    print("   - Any code calling add_work_income() must pass evaluation_score")
    print("   - Any code calling evaluate_artifact() must unpack 4 values")
    print("   - Task lifecycle management must call start_task()/end_task()")
    
    print("\n‚úÖ Handled In This Update:")
    print("   ‚úì livebench/agent/live_agent.py")
    print("   ‚úì livebench/tools/direct_tools.py")
    print("   ‚úì livebench/tools/tool_livebench.py")
    print("   ‚úì livebench/work/evaluator.py (already returned score)")
    
    print("\n" + "="*70)


if __name__ == "__main__":
    print("\n" + "="*70)
    print("ECONOMIC TRACKER V2 - COMPREHENSIVE VALIDATION")
    print("="*70)
    
    all_passed = True
    
    try:
        # Run all validations
        all_passed &= validate_integration_points()
        all_passed &= validate_threshold_logic()
        all_passed &= validate_cost_channel_separation()
        all_passed &= validate_query_capabilities()
        
        # Show demonstration
        demo_new_format()
        
        # Show compatibility notes
        check_backward_compatibility_notes()
        
        # Final summary
        print("\n" + "="*70)
        if all_passed:
            print("üéâ COMPREHENSIVE VALIDATION COMPLETE - ALL PASSED!")
            print("="*70)
            print("\n‚úÖ Implementation Summary:")
            print("   ‚Ä¢ Task-based recording with date indexing")
            print("   ‚Ä¢ Separate cost channels (LLM, Search, OCR, Other)")
            print("   ‚Ä¢ 0.6 evaluation threshold for payments")
            print("   ‚Ä¢ Comprehensive analytics and querying")
            print("   ‚Ä¢ All integration points updated")
            print("\nüìä The economic system is ready for production use.")
        else:
            print("‚ùå SOME VALIDATIONS FAILED")
            print("="*70)
            sys.exit(1)
        print()
        
    except Exception as e:
        print(f"\n‚ùå VALIDATION ERROR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
