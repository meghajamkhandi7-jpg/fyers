#!/usr/bin/env python3
"""
Estimate Task Completion Hours for GDPVal Dataset

This script iterates through all tasks in the GDPVal dataset and uses GPT-5.2
to estimate how many hours a professional human in the domain would need to
complete each task. The estimates are realistic and come with detailed reasoning.
"""

import os
import json
import pandas as pd
from openai import OpenAI
from pathlib import Path
from typing import Dict, Any
import time
from datetime import datetime

# OpenAI client (initialized lazily when needed)
client = None

# Configuration
MODEL = "gpt-5.2"
DATA_PATH = "../gdpval/data/train-00000-of-00001.parquet"
OUTPUT_DIR = "./task_hour_estimates"
OUTPUT_FILE = "task_hours.jsonl"
LOG_FILE = "./task_hour_estimation.log"

def log_message(message: str):
    """Log messages to both console and file"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"[{timestamp}] {message}"
    print(log_entry)
    with open(LOG_FILE, "a") as f:
        f.write(log_entry + "\n")

def load_gdpval_data() -> pd.DataFrame:
    """Load the gdpval parquet file"""
    log_message(f"Loading data from {DATA_PATH}")
    df = pd.read_parquet(DATA_PATH)
    log_message(f"Loaded {len(df)} tasks across {df['occupation'].nunique()} occupations")
    return df

def create_hour_estimation_prompt(
    task_id: str,
    occupation: str,
    sector: str,
    task_prompt: str,
    reference_files: list
) -> str:
    """
    Create the prompt that asks GPT-5.2 to estimate hours needed for a task
    """

    ref_files_text = ""
    if reference_files and len(reference_files) > 0:
        ref_files_text = f"\n**Reference Files Provided:** {', '.join(reference_files)}"

    estimation_prompt = f"""You are an expert workforce analyst with deep knowledge of professional work across all industries. Your task is to provide a realistic, well-reasoned estimate of how many hours a competent, experienced professional in the domain would need to complete the following task.

**TASK DETAILS:**
- **Task ID:** {task_id}
- **Occupation:** {occupation}
- **Sector:** {sector}
- **Task Description:**
{task_prompt}{ref_files_text}

**YOUR TASK:**
Estimate how many hours an experienced professional in the "{occupation}" field would realistically need to complete this task to a professional standard.

**CRITICAL ASSUMPTIONS:**

1. **Experienced Professional**: The person is an experienced professional (3-5+ years) who regularly does this type of work. They are efficient, know the tools, and don't need to learn basics. They work at a normal professional pace - efficient but not rushed.

2. **Focused Work Time**: Assume continuous, focused work without meetings or interruptions. This is pure "hands-on-keyboard" or "hands-on-work" time, not calendar time.

3. **Tools and Resources Available**: They have immediate access to all necessary tools, software, templates, and resources commonly used in this profession. No setup or installation time needed.

4. **Standard Professional Quality**: Work should be competent and professional, suitable for delivery - but not over-polished or gold-plated. Think "good enough for the client/boss to approve" not "award-winning masterpiece."

5. **Real-World Efficiency**: Experienced professionals are fast. They:
   - Know templates and shortcuts
   - Can quickly assess what's needed
   - Work efficiently without perfectionism
   - Reuse patterns from previous similar work
   - Don't overthink or over-research

6. **Task Scope**: Consider ONLY what's explicitly requested. Don't pad time for "nice to have" additions or extras beyond the requirements.

**REALISTIC HOUR RANGES** (be conservative - most tasks fall in lower ranges):
- **0.25-1 hour**: Quick tasks (simple formatting, basic data entry, routine template work)
- **1-3 hours**: Standard tasks that professionals do regularly (routine reports, standard analysis, basic presentations)
- **3-6 hours**: Moderately complex tasks requiring some depth (detailed analysis, comprehensive reports, multi-part deliverables)
- **6-12 hours**: Complex, multi-faceted work (major reports, advanced analysis requiring significant research, strategic planning documents)
- **12+ hours**: RARE - only for genuinely extensive projects (comprehensive research studies, complex multi-deliverable projects)

**IMPORTANT**: Most professional tasks take 1-6 hours. Be skeptical if you're estimating over 8 hours - make sure it's truly that complex and can't be done more efficiently.

**OUTPUT FORMAT:**
Provide your response as a structured JSON object with the following fields:

```json
{{
  "task_id": "{task_id}",
  "task_summary": "A concise 1-2 sentence summary of what the task requires",
  "complexity_factors": [
    "Key factor 1 that affects time estimate",
    "Key factor 2 that affects time estimate",
    "..."
  ],
  "reasoning": "Detailed explanation of your time estimate. Walk through the main steps required and why each takes the time you've estimated. Be specific and realistic - remember this is an experienced professional working efficiently.",
  "hours_estimate": 3.5,
  "confidence_level": "high|medium|low",
  "confidence_explanation": "Why you have this confidence level in your estimate"
}}
```

**CRITICAL REMINDERS**:
- BE REALISTIC - experienced professionals are faster than you think
- Most tasks take 1-6 hours of actual work time
- The hours_estimate should be a number (can include decimals like 2.5 or 0.5)
- Think "efficient professional pace" not "academic research pace"
- Consider: Would this really take more than a work day? If yes, why?
- Avoid padding - estimate the actual focused work time needed

Please provide your realistic hour estimate now."""

    return estimation_prompt

def estimate_hours_for_task(
    task_id: str,
    occupation: str,
    sector: str,
    task_prompt: str,
    reference_files: list
) -> Dict[str, Any]:
    """
    Use GPT-5.2 to estimate hours needed for a specific task
    """
    global client
    if client is None:
        client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

    log_message(f"Estimating hours for task: {task_id}")

    # Create the estimation prompt
    estimation_prompt = create_hour_estimation_prompt(
        task_id=task_id,
        occupation=occupation,
        sector=sector,
        task_prompt=task_prompt,
        reference_files=reference_files
    )

    try:
        # Call GPT-5.2
        log_message(f"Calling GPT-5.2 for task: {task_id}")
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert workforce analyst specializing in realistic task time estimation across all professional domains. You understand that experienced professionals are efficient and fast at their work. You provide conservative, realistic hour estimates based on actual focused work time (not padded calendar time). Most professional tasks take 1-6 hours."
                },
                {
                    "role": "user",
                    "content": estimation_prompt
                }
            ],
            response_format={"type": "json_object"}
        )

        # Parse the response
        result = json.loads(response.choices[0].message.content)

        # Add metadata
        result['metadata'] = {
            'task_id': task_id,
            'occupation': occupation,
            'sector': sector,
            'estimated_at': datetime.now().isoformat(),
            'model': MODEL,
            'prompt_tokens': response.usage.prompt_tokens,
            'completion_tokens': response.usage.completion_tokens,
            'total_tokens': response.usage.total_tokens
        }

        log_message(f"‚úÖ Successfully estimated hours for {task_id}: {result.get('hours_estimate', 'N/A')} hours (tokens: {response.usage.total_tokens})")
        return result

    except Exception as e:
        log_message(f"‚ùå ERROR estimating hours for {task_id}: {str(e)}")
        raise

def load_existing_estimates(output_file: Path) -> set:
    """Load already processed task IDs from the output file"""
    processed_ids = set()
    if output_file.exists():
        with open(output_file, 'r') as f:
            for line in f:
                try:
                    data = json.loads(line.strip())
                    task_id = data.get('task_id') or data.get('metadata', {}).get('task_id')
                    if task_id:
                        processed_ids.add(task_id)
                except Exception as e:
                    log_message(f"Warning: Could not parse line in existing output: {str(e)}")
    return processed_ids

def save_estimate(estimate: Dict[str, Any], output_file: Path):
    """Append the estimate to the JSONL output file"""
    with open(output_file, 'a') as f:
        f.write(json.dumps(estimate) + "\n")

def generate_summary_report(output_file: Path):
    """Generate a summary report of all estimates"""
    log_message("Generating summary report...")

    estimates = []
    with open(output_file, 'r') as f:
        for line in f:
            try:
                estimates.append(json.loads(line.strip()))
            except:
                pass

    if not estimates:
        log_message("No estimates to summarize")
        return

    # Calculate statistics
    hours = [e.get('hours_estimate', 0) for e in estimates]
    total_hours = sum(hours)
    avg_hours = total_hours / len(hours) if hours else 0
    min_hours = min(hours) if hours else 0
    max_hours = max(hours) if hours else 0

    # Token usage
    total_tokens = sum(e.get('metadata', {}).get('total_tokens', 0) for e in estimates)

    # Group by occupation
    occupation_stats = {}
    for e in estimates:
        occ = e.get('metadata', {}).get('occupation', 'Unknown')
        if occ not in occupation_stats:
            occupation_stats[occ] = {'count': 0, 'total_hours': 0}
        occupation_stats[occ]['count'] += 1
        occupation_stats[occ]['total_hours'] += e.get('hours_estimate', 0)

    # Create summary
    summary = {
        "generation_date": datetime.now().isoformat(),
        "model_used": MODEL,
        "total_tasks_estimated": len(estimates),
        "total_estimated_hours": round(total_hours, 2),
        "average_hours_per_task": round(avg_hours, 2),
        "min_hours": round(min_hours, 2),
        "max_hours": round(max_hours, 2),
        "total_tokens_used": total_tokens,
        "estimated_cost_usd": round((total_tokens / 1000000) * 5.0, 2),
        "occupation_breakdown": [
            {
                "occupation": occ,
                "task_count": stats['count'],
                "total_hours": round(stats['total_hours'], 2),
                "avg_hours": round(stats['total_hours'] / stats['count'], 2)
            }
            for occ, stats in sorted(occupation_stats.items(), key=lambda x: x[1]['total_hours'], reverse=True)
        ]
    }

    # Save summary
    summary_path = output_file.parent / "summary.json"
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2)

    log_message(f"üìä Summary Report:")
    log_message(f"   Total tasks: {len(estimates)}")
    log_message(f"   Total estimated hours: {summary['total_estimated_hours']:,.1f}")
    log_message(f"   Average hours per task: {summary['average_hours_per_task']:.2f}")
    log_message(f"   Range: {summary['min_hours']:.1f} - {summary['max_hours']:.1f} hours")
    log_message(f"   Total tokens: {total_tokens:,}")
    log_message(f"   Estimated cost: ${summary['estimated_cost_usd']:.2f}")
    log_message(f"   Summary saved to: {summary_path}")

def main():
    """Main execution function"""
    log_message("=" * 80)
    log_message("Starting Task Hour Estimation")
    log_message("=" * 80)

    # Check for API key
    if not os.environ.get("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY environment variable not set!")

    # Create output directory
    output_dir = Path(OUTPUT_DIR)
    output_dir.mkdir(exist_ok=True)
    output_file = output_dir / OUTPUT_FILE
    log_message(f"Output file: {output_file.absolute()}")

    # Load data
    df = load_gdpval_data()

    # Load existing estimates to resume if interrupted
    processed_ids = load_existing_estimates(output_file)
    if processed_ids:
        log_message(f"Found {len(processed_ids)} already processed tasks - will skip these")

    # Process each task
    total_tasks = len(df)
    skipped_count = len(processed_ids)
    processed_count = 0
    failed_count = 0

    for idx, row in df.iterrows():
        task_id = row.get('task_id', f"task_{idx}")

        log_message(f"\n{'=' * 80}")
        log_message(f"Processing task {idx + 1}/{total_tasks}: {task_id}")
        log_message(f"{'=' * 80}")

        # Check if already processed
        if task_id in processed_ids:
            log_message(f"‚è≠Ô∏è  SKIPPING {task_id} - already processed")
            continue

        try:
            # Extract task information
            occupation = row.get('occupation', 'Unknown')
            sector = row.get('sector', 'Unknown')
            task_prompt = str(row.get('prompt', ''))

            # Handle reference_files
            ref_files = row.get('reference_files', [])
            if isinstance(ref_files, (list, tuple)):
                ref_files = list(ref_files)
            elif hasattr(ref_files, '__iter__') and not isinstance(ref_files, str):
                ref_files = list(ref_files)
            else:
                ref_files = []

            # Generate hour estimate
            estimate = estimate_hours_for_task(
                task_id=task_id,
                occupation=occupation,
                sector=sector,
                task_prompt=task_prompt,
                reference_files=ref_files
            )

            # Save to file immediately (streaming approach)
            save_estimate(estimate, output_file)
            processed_count += 1

            # Rate limiting: sleep between requests
            if idx + 1 < total_tasks:
                sleep_time = 1  # 1 second between requests
                log_message(f"üí§ Sleeping for {sleep_time} seconds...")
                time.sleep(sleep_time)

        except Exception as e:
            log_message(f"‚ùå FAILED to process task {task_id}: {str(e)}")
            failed_count += 1
            continue

    # Generate summary report
    log_message(f"\n{'=' * 80}")
    log_message("Generating final summary report")
    log_message(f"{'=' * 80}")
    generate_summary_report(output_file)

    log_message(f"\n{'=' * 80}")
    log_message(f"‚úÖ Task Hour Estimation Complete!")
    log_message(f"{'=' * 80}")
    log_message(f"Total tasks in dataset: {total_tasks}")
    log_message(f"Already processed (skipped): {skipped_count}")
    log_message(f"Newly processed: {processed_count}")
    log_message(f"Failed: {failed_count}")
    log_message(f"Total estimates in output: {skipped_count + processed_count}")
    log_message(f"Output file: {output_file.absolute()}")
    log_message(f"{'=' * 80}")

if __name__ == "__main__":
    main()
