"""
Work Evaluator - Evaluates work artifacts and awards payment
"""

import os
import json
from typing import Dict, Optional, Tuple, List
from datetime import datetime


class WorkEvaluator:
    """
    Evaluates submitted work artifacts and determines payment.

    Uses LLM-based evaluation with category-specific rubrics:
    - Loads occupation-specific evaluation criteria from eval/meta_prompts/
    - Evaluates artifacts using GPT-4o against comprehensive rubrics
    - Scores on 0.0-1.0 scale with detailed feedback
    - No fallback - evaluation fails explicitly if LLM unavailable
    """

    def __init__(
        self,
        max_payment: float = 50.0,
        data_path: str = "./data/agent_data",
        use_llm_evaluation: bool = True,
        meta_prompts_dir: str = "./eval/meta_prompts"
    ):
        """
        Initialize Work Evaluator

        Args:
            max_payment: Maximum payment for perfect work
            data_path: Path to agent data directory
            use_llm_evaluation: Must be True (no fallback supported)
            meta_prompts_dir: Path to evaluation meta-prompts directory
        """
        self.max_payment = max_payment
        self.data_path = data_path
        self.use_llm_evaluation = use_llm_evaluation
        
        # Initialize LLM evaluator - required, will raise error if fails
        if not use_llm_evaluation:
            raise ValueError(
                "use_llm_evaluation must be True. "
                "Heuristic evaluation is no longer supported."
            )
        
        from .llm_evaluator import LLMEvaluator
        self.llm_evaluator = LLMEvaluator(
            meta_prompts_dir=meta_prompts_dir,
            max_payment=max_payment
        )
        print("âœ… LLM-based evaluation enabled (strict mode - no fallback)")

    def evaluate_artifact(
        self,
        signature: str,
        task: Dict,
        artifact_path: str,
        description: str = ""
    ) -> Tuple[bool, float, str, float]:
        """
        Evaluate a work artifact and determine payment

        Args:
            signature: Agent signature
            task: Task dictionary (must include 'max_payment' field)
            artifact_path: Path to submitted artifact (or list of paths)
            description: Agent's description of the work

        Returns:
            Tuple of (accepted, payment, feedback, evaluation_score)
        """
        # Handle both single path and list of paths
        artifact_paths = [artifact_path] if isinstance(artifact_path, str) else artifact_path

        # Check if artifacts exist
        if not any(os.path.exists(path) for path in artifact_paths):
            self._log_evaluation(
                signature=signature,
                task_id=task['task_id'],
                artifact_path=artifact_paths[0] if artifact_paths else "none",
                payment=0.0,
                feedback="Artifact file not found. No payment awarded.",
                evaluation_score=0.0
            )
            return (
                False,
                0.0,
                "Artifact file not found. No payment awarded.",
                0.0
            )

        # Get file info for primary artifact
        primary_path = artifact_paths[0]
        file_size = os.path.getsize(primary_path) if os.path.exists(primary_path) else 0
        file_ext = os.path.splitext(primary_path)[1].lower()

        # Basic checks
        if file_size == 0:
            self._log_evaluation(
                signature=signature,
                task_id=task['task_id'],
                artifact_path=primary_path,
                payment=0.0,
                feedback="Artifact file is empty. No payment awarded.",
                evaluation_score=0.0
            )
            return (
                False,
                0.0,
                "Artifact file is empty. No payment awarded.",
                0.0
            )

        # LLM evaluation only - no fallback
        if not self.use_llm_evaluation or not self.llm_evaluator:
            raise RuntimeError(
                "LLM evaluation is required but not properly configured. "
                "Ensure use_llm_evaluation=True and OPENAI_API_KEY is set."
            )

        # Get task-specific max payment (fallback to global if not set)
        task_max_payment = task.get('max_payment', self.max_payment)

        # Evaluate using LLM with task-specific max payment - let errors propagate
        evaluation_score, feedback, payment = self.llm_evaluator.evaluate_artifact(
            task=task,
            artifact_paths=artifact_paths,
            description=description,
            max_payment=task_max_payment
        )

        # Log LLM evaluation
        self._log_evaluation(
            signature=signature,
            task_id=task['task_id'],
            artifact_path=artifact_paths,  # Pass all paths, not just primary
            payment=payment,
            feedback=feedback,
            evaluation_score=evaluation_score,
            evaluation_method="llm"
        )

        accepted = payment > 0
        return (accepted, payment, feedback, evaluation_score)

    # REMOVED: Heuristic evaluation methods
    # All evaluation now uses LLM with category-specific rubrics
    # No fallback to ensure evaluation quality and consistency

    def _log_evaluation(
        self,
        signature: str,
        task_id: str,
        artifact_path,  # Can be str or list
        payment: float,
        feedback: str,
        evaluation_score: Optional[float] = None,
        evaluation_method: str = "heuristic"
    ) -> None:
        """Log evaluation result
        
        Args:
            artifact_path: Single path (str) or list of paths
        """
        # Ensure directory exists
        eval_log_dir = os.path.join(self.data_path, "work")
        os.makedirs(eval_log_dir, exist_ok=True)

        eval_log_file = os.path.join(eval_log_dir, "evaluations.jsonl")

        # Normalize artifact_path to list for consistent logging
        if isinstance(artifact_path, str):
            artifact_paths_list = [artifact_path]
        else:
            artifact_paths_list = artifact_path

        # Create log entry with all artifact paths
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "task_id": task_id,
            "artifact_path": artifact_path if isinstance(artifact_path, str) else artifact_path,  # Keep original format
            "artifact_paths": artifact_paths_list,  # Always include list format for clarity
            "payment": payment,
            "feedback": feedback,
            "evaluation_score": evaluation_score,  # 0.0-1.0 scale
            "evaluation_method": evaluation_method  # "llm" or "heuristic"
        }

        # Append to log file
        with open(eval_log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry) + "\n")

    def get_evaluation_history(self, signature: str) -> list:
        """
        Get evaluation history for an agent

        Args:
            signature: Agent signature

        Returns:
            List of evaluation records
        """
        eval_log_file = os.path.join(
            self.data_path,
            signature,
            "work",
            "evaluations.jsonl"
        )

        if not os.path.exists(eval_log_file):
            return []

        evaluations = []
        with open(eval_log_file, "r") as f:
            for line in f:
                evaluations.append(json.loads(line))

        return evaluations

    def get_total_earnings(self, signature: str) -> float:
        """
        Get total work earnings for an agent

        Args:
            signature: Agent signature

        Returns:
            Total earnings
        """
        evaluations = self.get_evaluation_history(signature)
        return sum(eval['payment'] for eval in evaluations)

    def __str__(self) -> str:
        return f"WorkEvaluator(max_payment=${self.max_payment})"
