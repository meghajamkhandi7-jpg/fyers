#!/usr/bin/env python3
"""
Test script to generate meta-prompt for a single category.
Useful for testing before running the full generation.
"""

import os
import sys
import json
import pandas as pd
from pathlib import Path

# Add the current directory to the path to import from generate_meta_prompts
sys.path.insert(0, str(Path(__file__).parent))

from generate_meta_prompts import (
    load_gdpval_data,
    generate_meta_prompt_for_category,
    log_message,
    OUTPUT_DIR,
    get_safe_filename
)

def test_single_category(category_name: str = None):
    """Test meta-prompt generation for a single category"""
    
    # Check for API key
    if not os.environ.get("OPENAI_API_KEY"):
        print("ERROR: OPENAI_API_KEY environment variable not set!")
        print("Please set it with: export OPENAI_API_KEY='your-api-key-here'")
        return False
    
    # Load data
    print("Loading gdpval data...")
    df = load_gdpval_data()
    
    # Get categories
    categories = sorted(df['occupation'].unique())
    print(f"\nAvailable categories ({len(categories)} total):")
    for i, cat in enumerate(categories, 1):
        print(f"  {i:2d}. {cat}")
    
    # Select category
    if category_name:
        if category_name not in categories:
            print(f"\nERROR: Category '{category_name}' not found!")
            return False
        selected_category = category_name
    else:
        # Use the first category as default
        selected_category = categories[0]
        print(f"\nNo category specified, using first category: {selected_category}")
    
    print(f"\n{'='*80}")
    print(f"Testing meta-prompt generation for: {selected_category}")
    print(f"{'='*80}\n")
    
    # Filter data for this category
    category_data = df[df['occupation'] == selected_category]
    print(f"Tasks in this category: {len(category_data)}")
    
    # Generate meta-prompt
    try:
        meta_prompt_data = generate_meta_prompt_for_category(selected_category, category_data)
        
        # Create test output directory
        test_output_dir = Path(OUTPUT_DIR) / "test"
        test_output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save result
        safe_name = get_safe_filename(selected_category)
        output_file = test_output_dir / f"{safe_name}_test.json"
        
        with open(output_file, 'w') as f:
            json.dump(meta_prompt_data, f, indent=2)
        
        print(f"\n{'='*80}")
        print(f"SUCCESS! Generated meta-prompt for: {selected_category}")
        print(f"Output saved to: {output_file}")
        print(f"{'='*80}\n")
        
        # Print summary
        metadata = meta_prompt_data.get('metadata', {})
        print("Summary:")
        print(f"  - Tokens used: {metadata.get('total_tokens', 'N/A')}")
        print(f"  - Model: {metadata.get('model', 'N/A')}")
        print(f"  - Generated at: {metadata.get('generated_at', 'N/A')}")
        
        # Print a preview of the evaluation prompt
        if 'evaluation_prompt' in meta_prompt_data:
            prompt_preview = meta_prompt_data['evaluation_prompt'][:500]
            print(f"\nEvaluation Prompt Preview:")
            print(f"{prompt_preview}...")
        
        return True
        
    except Exception as e:
        print(f"\n{'='*80}")
        print(f"ERROR: Failed to generate meta-prompt")
        print(f"{'='*80}")
        print(f"Error details: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    # Check if category name provided as argument
    category = sys.argv[1] if len(sys.argv) > 1 else None
    
    success = test_single_category(category)
    sys.exit(0 if success else 1)

