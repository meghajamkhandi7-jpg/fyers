"""
TrackedProvider â€” wraps a nanobot LLMProvider to feed token usage
into ClawWork's EconomicTracker on every chat() call.

Nanobot's LLMResponse.usage already provides accurate prompt_tokens and
completion_tokens (extracted from litellm), so this is a direct
improvement over ClawWork's original `len(text) // 4` estimation.
"""

from __future__ import annotations

from typing import Any

from nanobot.providers.base import LLMProvider, LLMResponse


class TrackedProvider:
    """Transparent wrapper that tracks token costs via EconomicTracker."""

    def __init__(self, provider: LLMProvider, tracker: Any) -> None:
        self._provider = provider
        self._tracker = tracker  # EconomicTracker

    async def chat(
        self,
        messages: list[dict[str, Any]],
        tools: list[dict[str, Any]] | None = None,
        model: str | None = None,
        max_tokens: int = 4096,
        temperature: float = 0.7,
    ) -> LLMResponse:
        response = await self._provider.chat(
            messages=messages,
            tools=tools,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
        )

        # Feed usage into EconomicTracker
        if response.usage and self._tracker:
            self._tracker.track_tokens(
                response.usage.get("prompt_tokens", 0),
                response.usage.get("completion_tokens", 0),
            )

        return response

    # Forward everything else to the real provider
    def __getattr__(self, name: str) -> Any:
        return getattr(self._provider, name)
