#!/usr/bin/env python3
"""
Generate Meta-Prompts for LLM-based Evaluation

This script iterates through all 44 task categories (occupations) in the gdpval dataset
and uses GPT-4o to generate detailed prompts and guidelines for evaluating task outputs
in each category.

The meta-prompts generated are category-specific (not task-specific) and designed to help
LLMs evaluate the file-based outputs/artifacts produced for tasks in that category.
"""

import os
import json
import pandas as pd
from openai import OpenAI
from pathlib import Path
from typing import List, Dict, Any
import time
from datetime import datetime

# OpenAI client (initialized lazily when needed)
client = None

# Configuration
MODEL = "gpt-5.2"
DATA_PATH = "../gdpval/data/train-00000-of-00001.parquet"
OUTPUT_DIR = "./meta_prompts"
LOG_FILE = "./meta_prompt_generation.log"

def log_message(message: str):
    """Log messages to both console and file"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"[{timestamp}] {message}"
    print(log_entry)
    with open(LOG_FILE, "a") as f:
        f.write(log_entry + "\n")

def load_gdpval_data() -> pd.DataFrame:
    """Load the gdpval parquet file"""
    log_message(f"Loading data from {DATA_PATH}")
    df = pd.read_parquet(DATA_PATH)
    log_message(f"Loaded {len(df)} tasks across {df['occupation'].nunique()} occupations")
    return df

def create_meta_prompt_generation_request(
    category: str,
    sector: str,
    task_prompts: List[str],
    sample_task_details: List[Dict[str, Any]]
) -> str:
    """
    Create the meta-prompt that asks GPT-4o to generate evaluation guidelines
    for a specific category
    """
    
    # Include 2-3 sample tasks with their details for context
    sample_tasks_text = ""
    for i, task in enumerate(sample_task_details[:3], 1):
        sample_tasks_text += f"\n### Sample Task {i}:\n"
        sample_tasks_text += f"**Prompt:**\n{task['prompt']}\n\n"
        ref_files = task.get('reference_files', [])
        if ref_files is not None and len(ref_files) > 0:
            sample_tasks_text += f"**Reference Files:** {', '.join(ref_files)}\n"
    
    meta_prompt = f"""You are an expert evaluator designing evaluation guidelines for AI-generated work outputs.

**CONTEXT:**
You are analyzing tasks from the occupation category: "{category}"
Primary sector: {sector}

This category has {len(task_prompts)} tasks in total. Below are some representative sample tasks from this category to give you context:

{sample_tasks_text}

**YOUR TASK:**
Generate comprehensive, detailed evaluation prompts and guidelines that can be used by LLMs to assess the quality and correctness of outputs/artifacts produced for tasks in the "{category}" category.

**CRITICAL REQUIREMENT:**
The evaluation system uses a **0-10 scoring scale** where missing or incomplete deliverables MUST receive scores of 0-2. This is non-negotiable - if required output files are missing or work is severely incomplete, it is unacceptable regardless of the quality of what was delivered.

**IMPORTANT REQUIREMENTS:**

1. **Category-Level Generality**: The guidelines should apply to ALL tasks in this category, not just specific tasks. However, you may use partial information from the sample tasks as illustrative examples in your prompts.

2. **File-Based Evaluation**: The evaluation process will receive:
   - The original task prompt
   - The reference files (inputs) mentioned in the task
   - The OUTPUT FILES/ARTIFACTS produced by an agent attempting the task
   
   Your guidelines must instruct the evaluator LLM on how to assess these output files.

3. **CRITICAL - Missing Artifacts**: If ANY required output files are missing or if the work is incomplete, the evaluator MUST assign a score of 0-2. Missing deliverables are unacceptable and should receive the lowest scores.

4. **Comprehensive Criteria**: Include guidelines for evaluating:
   - **Completeness** (MOST IMPORTANT): All required output files exist, all requirements addressed
   - **Correctness**: Accuracy of data, calculations, information
   - **Quality**: Professional formatting, clarity, organization
   - **Domain-Specific Standards**: Industry-specific best practices for this occupation

5. **Scoring Scale (0-10)**:
   - **0-2**: Unacceptable - Missing required output files, severely incomplete work
   - **3-4**: Poor - Major issues, many requirements unmet
   - **5-6**: Acceptable - Notable gaps or errors
   - **7-8**: Good - Minor issues only
   - **9-10**: Excellent - Complete, accurate, professional

6. **Structured Output**: Provide:
   - A clear evaluation framework/rubric with 0-10 scale
   - Specific things to look for in the output files
   - Common failure modes that result in low scores
   - Explicit criteria for automatic low scores (missing files, incomplete work)

7. **Actionable Instructions**: Make the guidelines concrete and actionable so an LLM evaluator can follow them systematically.

**OUTPUT FORMAT:**
Provide your response as a structured JSON object with the following fields:

```json
{{
  "category": "{category}",
  "evaluation_prompt": "A detailed prompt that will be given to an LLM evaluator, explaining what they need to do and how to evaluate the outputs",
  "evaluation_rubric": {{
    "completeness": {{
      "weight": 0.40,
      "description": "All required output files exist and all task requirements are addressed",
      "criteria": ["specific criterion 1", "criterion 2", ...],
      "scoring_guidance": "0-2 if files missing or severely incomplete, 3-4 if many requirements unmet, 5-6 if notable gaps, 7-8 if minor omissions, 9-10 if fully complete"
    }},
    "correctness": {{
      "weight": 0.30,
      "description": "Accuracy of data, calculations, information, and logic",
      "criteria": ["specific criterion 1", "criterion 2", ...],
      "scoring_guidance": "0-10 scale based on accuracy of content"
    }},
    "quality": {{
      "weight": 0.20,
      "description": "Professional formatting, clarity, organization",
      "criteria": ["specific criterion 1", "criterion 2", ...],
      "scoring_guidance": "0-10 scale based on presentation quality"
    }},
    "domain_standards": {{
      "weight": 0.10,
      "description": "Industry-specific best practices for this occupation",
      "criteria": ["specific criterion 1", "criterion 2", ...],
      "scoring_guidance": "0-10 scale based on professional standards adherence"
    }}
  }},
  "file_inspection_checklist": [
    "What to check in output file 1",
    "What to check in output file 2",
    ...
  ],
  "common_failure_modes": [
    "Common mistake 1",
    "Common mistake 2",
    ...
  ],
  "scoring_guidelines": {{
    "overall_approach": "Calculate weighted average: completeness (40%), correctness (30%), quality (20%), domain_standards (10%). CRITICAL: If any required files are missing, override final score to 0-2 regardless of other dimensions.",
    "score_scale": "0-10 where 0-2=Unacceptable (missing files/incomplete), 3-4=Poor, 5-6=Acceptable, 7-8=Good, 9-10=Excellent",
    "automatic_low_score_triggers": [
      "Required output files are missing",
      "Deliverables are severely incomplete",
      "Major requirements from prompt are not addressed"
    ],
    "excellent_output_characteristics": ["All files present", "Complete and accurate", "Professional quality", ...],
    "poor_output_characteristics": ["Missing required files", "Incomplete work", "Major errors", ...]
  }},
  "example_evaluation_questions": [
    "Specific question evaluator should ask about the output",
    "Another question",
    ...
  ]
}}
```

Please generate comprehensive, thoughtful evaluation guidelines for the "{category}" category."""

    return meta_prompt

def generate_meta_prompt_for_category(
    category: str,
    category_data: pd.DataFrame
) -> Dict[str, Any]:
    """
    Use GPT-4o to generate evaluation guidelines for a specific category
    """
    global client
    if client is None:
        client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    
    log_message(f"Generating meta-prompt for category: {category}")
    
    # Extract information from the category data
    sector = category_data['sector'].iloc[0]
    task_prompts = category_data['prompt'].tolist()
    
    # Prepare sample task details
    sample_task_details = []
    for idx, row in category_data.head(3).iterrows():
        # Safely extract reference files and convert to list
        ref_files = row.get('reference_files', [])
        if isinstance(ref_files, (list, tuple)):
            ref_files = list(ref_files)
        elif hasattr(ref_files, '__iter__') and not isinstance(ref_files, str):
            ref_files = list(ref_files)
        else:
            ref_files = []
        
        sample_task_details.append({
            'prompt': str(row['prompt']),
            'reference_files': ref_files
        })
    
    # Create the meta-prompt
    meta_prompt = create_meta_prompt_generation_request(
        category=category,
        sector=sector,
        task_prompts=task_prompts,
        sample_task_details=sample_task_details
    )
    
    try:
        # Call GPT-4o
        log_message(f"Calling GPT-4o for category: {category}")
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert in creating evaluation frameworks and rubrics for assessing work quality across different professional domains. You design rubrics that heavily penalize incomplete or missing deliverables, using a 0-10 scale where missing artifacts result in scores of 0-2."
                },
                {
                    "role": "user",
                    "content": meta_prompt
                }
            ],
            # temperature=0.7,
            # max_tokens=4000,
            response_format={"type": "json_object"}
        )
        
        # Parse the response
        result = json.loads(response.choices[0].message.content)
        
        # Add metadata
        result['metadata'] = {
            'category': category,
            'sector': sector,
            'num_tasks_in_category': len(category_data),
            'generated_at': datetime.now().isoformat(),
            'model': MODEL,
            'prompt_tokens': response.usage.prompt_tokens,
            'completion_tokens': response.usage.completion_tokens,
            'total_tokens': response.usage.total_tokens
        }
        
        log_message(f"Successfully generated meta-prompt for {category} (tokens: {response.usage.total_tokens})")
        return result
        
    except Exception as e:
        log_message(f"ERROR generating meta-prompt for {category}: {str(e)}")
        raise

def get_safe_filename(category: str) -> str:
    """Convert category name to safe filename"""
    return category.replace("/", "-").replace(",", "").replace(" ", "_")

def category_already_generated(category: str, output_dir: Path) -> bool:
    """Check if a category has already been generated"""
    safe_category_name = get_safe_filename(category)
    filename = f"{safe_category_name}.json"
    filepath = output_dir / filename
    return filepath.exists()

def save_meta_prompt(category: str, meta_prompt_data: Dict[str, Any], output_dir: Path):
    """Save the generated meta-prompt to a JSON file"""
    safe_category_name = get_safe_filename(category)
    filename = f"{safe_category_name}.json"
    filepath = output_dir / filename
    
    with open(filepath, 'w') as f:
        json.dump(meta_prompt_data, f, indent=2)
    
    log_message(f"Saved meta-prompt to {filepath}")

def generate_summary_report(output_dir: Path, all_results: List[Dict[str, Any]]):
    """Generate a summary report of all generated meta-prompts"""
    summary = {
        "generation_date": datetime.now().isoformat(),
        "total_categories": len(all_results),
        "model_used": MODEL,
        "categories": []
    }
    
    total_tokens = 0
    for result in all_results:
        metadata = result['metadata']
        summary['categories'].append({
            'category': metadata['category'],
            'sector': metadata['sector'],
            'num_tasks': metadata['num_tasks_in_category'],
            'tokens_used': metadata['total_tokens']
        })
        total_tokens += metadata['total_tokens']
    
    summary['total_tokens_used'] = total_tokens
    summary['estimated_cost_usd'] = (total_tokens / 1000000) * 5.0  # Rough estimate for GPT-4o
    
    summary_path = output_dir / "generation_summary.json"
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2)
    
    log_message(f"Generated summary report at {summary_path}")
    log_message(f"Total tokens used: {total_tokens}")
    log_message(f"Estimated cost: ${summary['estimated_cost_usd']:.2f}")

def main():
    """Main execution function"""
    log_message("="*80)
    log_message("Starting Meta-Prompt Generation")
    log_message("="*80)
    
    # Check for API key
    if not os.environ.get("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY environment variable not set!")
    
    # Create output directory
    output_dir = Path(OUTPUT_DIR)
    output_dir.mkdir(exist_ok=True)
    log_message(f"Output directory: {output_dir.absolute()}")
    
    # Load data
    df = load_gdpval_data()
    
    # Get all unique categories (occupations)
    categories = sorted(df['occupation'].unique())
    log_message(f"Found {len(categories)} categories to process")
    
    # Process each category
    all_results = []
    skipped_count = 0
    
    for i, category in enumerate(categories, 1):
        log_message(f"\n{'='*80}")
        log_message(f"Processing category {i}/{len(categories)}: {category}")
        log_message(f"{'='*80}")
        
        # Check if already generated
        if category_already_generated(category, output_dir):
            log_message(f"SKIPPING {category} - already generated")
            skipped_count += 1
            
            # Load existing data for summary
            try:
                safe_name = get_safe_filename(category)
                filepath = output_dir / f"{safe_name}.json"
                with open(filepath, 'r') as f:
                    existing_data = json.load(f)
                    all_results.append(existing_data)
            except Exception as e:
                log_message(f"Warning: Could not load existing file for {category}: {str(e)}")
            
            continue
        
        # Filter data for this category
        category_data = df[df['occupation'] == category]
        
        try:
            # Generate meta-prompt
            meta_prompt_data = generate_meta_prompt_for_category(category, category_data)
            
            # Save to file
            save_meta_prompt(category, meta_prompt_data, output_dir)
            
            all_results.append(meta_prompt_data)
            
            # Rate limiting: sleep between requests to avoid hitting API limits
            if i < len(categories):
                sleep_time = 2  # 2 seconds between requests
                log_message(f"Sleeping for {sleep_time} seconds...")
                time.sleep(sleep_time)
                
        except Exception as e:
            log_message(f"FAILED to process category {category}: {str(e)}")
            continue
    
    # Generate summary report
    log_message(f"\n{'='*80}")
    log_message("Generating summary report")
    log_message(f"{'='*80}")
    generate_summary_report(output_dir, all_results)
    
    log_message(f"\n{'='*80}")
    log_message(f"Meta-Prompt Generation Complete!")
    log_message(f"Total categories: {len(categories)}")
    log_message(f"Skipped (already generated): {skipped_count}")
    log_message(f"Newly generated: {len(all_results) - skipped_count}")
    log_message(f"Total in output: {len(all_results)}")
    log_message(f"Output directory: {output_dir.absolute()}")
    log_message(f"{'='*80}")

if __name__ == "__main__":
    main()

